# :rocket: Proyecto ETL - Ferrimac :hammer: :wrench:

## :memo: Descripci√≥n del Proyecto

Este proyecto ETL (Extract, Transform, Load) est√° dise√±ado para Ferrimac, una ferreter√≠a que necesita optimizar la gesti√≥n de su stock. Utiliza Apache Airflow para automatizar el proceso de extracci√≥n, transformaci√≥n y carga de datos desde una API y archivos locales. El objetivo principal es calcular y visualizar el stock valorizado en pesos, tomando en cuenta los productos en inventario, su precio en d√≥lares y la cotizaci√≥n diaria del d√≥lar.

El pipeline incluye m√∫ltiples tareas (tasks) en Airflow, responsables de extraer la informaci√≥n de ventas y compras, actualizar el stock y cargar los datos transformados en una base de datos Amazon Redshift.


## :file_folder: Estructura del Proyecto.


```bash

ETL_AIRFLOW
    ‚îú‚îÄ‚îÄ /.github/workflows                             #
    ‚îú‚îÄ‚îÄ /base_datos/                                   # Base de datos donde se almacenan archivos del negocio.
    ‚îú‚îÄ‚îÄ /config/                                       # Configuraci√≥n de Airflow.
    ‚îú‚îÄ‚îÄ /dags/
    ‚îÇ     ‚îú‚îÄ‚îÄ etl_update_stock_ferrimac.py             # Archivo principal del DAG de Airflow.
    ‚îú‚îÄ‚îÄ /logs/                                         # Carpeta donde persisten los logs de Airflow.
    ‚îú‚îÄ‚îÄ /functions_etl/
    ‚îÇ     ‚îú‚îÄ‚îÄ data_transform.py                        # Archivo que transforma los datos y valoriza el stock.
    ‚îÇ     ‚îú‚îÄ‚îÄ extract_file_purchases_to_stock.py       # Funci√≥n que extrae datos del sistema de compras.
    ‚îÇ     ‚îú‚îÄ‚îÄ extract_file_sells_to_stock.py           # Funci√≥n que extrae datos del sistema de ventas.
    ‚îÇ     ‚îú‚îÄ‚îÄ load_data.py                             # Funci√≥n que carga los datos en Redshift.
    ‚îÇ     ‚îú‚îÄ‚îÄ obtain_currency.py                       # Funci√≥n que obtiene la cotizaci√≥n del d√≥lar.
    ‚îÇ     ‚îú‚îÄ‚îÄ update_stock.py                          # Funci√≥n que actualiza el stock en unidades.
    ‚îú‚îÄ‚îÄ /plugins/                                      # Directorio de configuraci√≥n de Airflow.
    ‚îú‚îÄ‚îÄ /test/                                         # Directorio con pruebas unitarias.
    ‚îú‚îÄ‚îÄ /utils/
    ‚îÇ     ‚îú‚îÄ‚îÄ config.py                                # Archivo de configuraci√≥n de variables de entorno.
    ‚îú‚îÄ‚îÄ .gitignore                                     # Archivo de configuraci√≥n de Git.
    ‚îú‚îÄ‚îÄ docker-compose.yml                             # Archivo de configuraci√≥n de Docker Compose.
    ‚îú‚îÄ‚îÄ README.md                                      # Documentaci√≥n del proyecto.
    ‚îî‚îÄ‚îÄ requirements.txt                               # Archivo con librer√≠as utilizadas en el proyecto.                                   
    
```



## :desktop_computer: :gear: Pasos de Configuraci√≥n.

## :memo: Pre requisitos.

:white_check_mark: *Python 3.8 o superior* üêç


:white_check_mark: *Apache Airflow* üå¨Ô∏è


:white_check_mark: *Docker y Docker Compose* üê≥

## Pasos

1- Clonar el repositorio.

```bash
# Comando para clonar el repositorio
git clone git@github.com:Simontiberio/ETL_airflow.git

```

2- Crear un archivo .env en el directorio principal del proyecto, incluyendo las credenciales de la API y las rutas de los archivos de ventas y compras.


```bash

# Credenciales Redshift.
REDSHIFT_USER= '2024_nombre_apellido'
REDSHIFT_PASSWORD='PASSWORD'
REDSHIFT_DB= 'NOMBRE_DB'
REDSHIFT_HOST='REDSHIFT_HOST'
REDSHIFT_PORT= PORT

# Variables de entorno.
REDSHIFT_SCHEMA = 'REDSHIFT_SCHEMA'
api_key = 'your_api_key'
compras_file = 'nombre_archivo.xlsx'
stock_file = 'nombre_archivo.xlsx'
ventas_file = 'nombre_archivo.xlsx'
monetized_stock_file = 'nombre_archivo.xlsx'
data_quotes = 'nombre_archivo.csv'
list_prices_file = 'nombre_archivo.xlsx'

```
3- Iniciar los servicios de Airflow utilizando Docker Compose:

```bash
docker-compose --env-file .env up -d
```

4- Acceder a la interfaz de Airflow en http://localhost:8080 y activar el DAG llamado etl_update_stock_ferrimac.


5- Credenciales de acceso

Usuario: airflow

Contrase√±a: airflow

Nota: Aseg√∫rate de que estas credenciales coincidan con las configuradas en tu archivo docker-compose.yml, si las has cambiado.

6- Activar el DAG

Despu√©s de iniciar sesi√≥n, activa el DAG llamado etl_update_stock_ferrimac para comenzar el proceso de ETL. Puedes encontrarlo en la lista de DAGs en la interfaz principal.



## Estructura del Pipeline.

El pipeline de Airflow se compone de tres fases principales:

:one: Extracci√≥n de Datos

En esta etapa, se recolectan los datos mediante tres tareas ejecutadas en paralelo:

``` append_to_data_price :``` Obtiene diariamente la cotizaci√≥n del d√≥lar desde una API, actualizando un archivo hist√≥rico de cotizaciones.

``` extract_file_sells_to_stock : ``` Extrae las ventas diarias del sistema de ventas al cierre de cada jornada.

``` extract_file_purchases_to_stock :``` Recupera las compras de productos a distintos proveedores, actualizando el inventario con las compras recepcionadas.

:two: Transformaci√≥n de Datos

Aqu√≠ se transforman los datos para calcular el valor del stock.

``` update_stock : ```: Actualiza el stock diario considerando las ventas y compras registradas.

``` monetize_stock :``` Valoriza el stock actualizado utilizando la √∫ltima lista de precios en d√≥lares y la cotizaci√≥n del d√≠a.

:three: Carga de Datos

En esta √∫ltima etapa, los datos son almacenados y cargados en Redshift.

``` load_data :``` Convierte el stock valorizado a formato Parquet para un almacenamiento m√°s eficiente.

``` load_data_to_Redshift :``` Carga el archivo Parquet a la base de datos Redshift, ubicada en un cluster de AWS.


## Representacion gr√°fica del DAG y sus dependencias entre tareas.

A continuaci√≥n, se visualiza las secuencias y dependencias de las tareas que componen el DAG. 

:warning: El DAG se encuentra programado para que inicie a las 10 p.m de Lunes a Sabados, entendiendo que para esa hora ya culmino la jornada laboral (``` schedule_interval='0 22 * * 1-6'```).



![Arquitectura y dependencias del Pipeline](./image.png)



## Diagrama Entidad Relaci√≥n de Ferrimac

A continuacion, se muestra como se modelo el subdomionio de mercaderias: 



![Diagrama Entidad Relacion ](./ferrimac_DER.png)


## üóÇÔ∏è Descripci√≥n del Modelo de Datos. üìù 

Este modelo de datos soporta la valorizaci√≥n y gesti√≥n de inventarios mediante el almacenamiento de informaci√≥n clave sobre productos, proveedores y registros diarios de actividades (ventas, compras y valorizaci√≥n de stock). La estructura de las tablas facilita un an√°lisis detallado y diario de cada aspecto de la gesti√≥n de inventarios.

Columnas de Producto y Proveedor:

Las primeras cinco columnas (id_product, model, descripcion, suppliers, y brands) contienen informaci√≥n descriptiva y de identificaci√≥n del producto, as√≠ como detalles del proveedor y marca.

Campos de Fecha (date):

Cada columna con un formato de fecha (por ejemplo, 2024-19-09) representa un snapshot diario de las operaciones, manteniendo un registro detallado de las actividades de cada d√≠a del mes.
Granularidad: Estas columnas de fecha representan valores diarios, con un snapshot diario de cada variable (como stock, unidades vendidas y compras). Esto permite analizar las variaciones diarias del inventario y calcular el valor del stock en funci√≥n de los movimientos de venta y compra, y la cotizaci√≥n diaria del d√≥lar.
Prop√≥sito de las fechas: Al incluir una columna por cada d√≠a del mes, es posible observar el comportamiento y cambios en el stock en un intervalo de tiempo espec√≠fico, facilitando tanto la consulta hist√≥rica como el an√°lisis de tendencias y la valorizaci√≥n precisa.


### ‚öôÔ∏è Pruebas Unitarias

Este proyecto cuenta con un conjunto de pruebas unitarias implementadas con la biblioteca `pytest`. Estas pruebas permiten verificar el correcto funcionamiento de funciones clave, asegurando que los datos se carguen y transformen adecuadamente y que la conexi√≥n con la API se realice de forma correcta. Ejecutar las pruebas de manera regular ayuda a detectar posibles errores o cambios en el comportamiento esperado, mejorando as√≠ la calidad y la confiabilidad del c√≥digo.

#### üîç Funcionalidad de las Pruebas Actuales

1. **üìÑ Prueba de carga de datos** (`test_load_data_file`): Valida si el archivo se carga correctamente en formato Excel, convirti√©ndolo en un archivo parquet con la estructura y ubicaci√≥n esperada.
2. **üí± Prueba de obtenci√≥n de cotizacion del dolar** (`test_obtain_currency`): Comprueba si se obtiene el diccionario con los datos correctos desde una API externa, validando la respuesta y estructura esperada.

#### üìã Requisitos Previos

Para ejecutar las pruebas unitarias, asegurarse de que `pytest` est√© instalado. Si no lo tienes, puedes instalarlo con:

```bash
pip install pytest
```

### üöÄ Instrucciones para Ejecutar las Pruebas

 1- Dir√≠gete al directorio ra√≠z del proyecto:

```bash
cd etl_airflow/test/
```

 2- Ejecuta todas las pruebas unitarias usando el comando:

```bash
pytest test_load_data_file.py
```

```bash
pytest obtain_currency.py
```

3- Si deseas ver detalles de cada prueba (modo detallado), ejecuta:

```bash
pytest -v test_load_data_file.py
```


## üõ†Ô∏è √Åreas de Mejora  üîß 

Para continuar evolucionando y mejorando el pipeline de datos desarrollado para Ferrimac, se identifican las siguientes oportunidades de mejora, que pueden brindar un valor agregado y permitir una gesti√≥n m√°s eficiente de las operaciones del negocio:

:one: Modelos Predictivos para anticiparse al comportamiento de las variables del negocio: 
La implementaci√≥n de modelos de machine learning que permitan predecir la volatilidad del tipo de cambio beneficiar√≠a la toma de decisiones financieras y de compra de inventario. Con estas proyecciones, Ferrimac podr√≠a optimizar sus compras y reducir riesgos asociados a fluctuaciones en la divisa, mejorando la precisi√≥n en la valorizaci√≥n de su stock y en la planificaci√≥n de costos.  Lo mismo aplica para las predicciones de los picos de demanda. Esto permitir√≠a a Ferrimac planificar inventarios y asegurar disponibilidad de productos en momentos clave, optimizando la cadena de suministro y reduciendo costos de oportunidad asociados a la falta de stock.


:two: Desarrollo de interfaces gr√°ficas para la visualizaci√≥n de datos : 
Incorporar interfaces gr√°ficas permitir√≠a a Ferrimac visualizar de manera intuitiva y en tiempo real la informaci√≥n clave sobre las variaciones del tipo de cambio y la valorizaci√≥n del stock. Estas visualizaciones facilitar√≠an la identificaci√≥n de tendencias y patrones de forma r√°pida y accesible, permitiendo al equipo tomar decisiones informadas y en tiempo adecuado. Adem√°s, una representaci√≥n visual de los datos aumentar√≠a la transparencia y comprensi√≥n de la evoluci√≥n del negocio, mejorando la comunicaci√≥n y el an√°lisis estrat√©gico.

Estas √°reas de mejora proporcionar√≠an una visi√≥n integral de los datos, convirti√©ndolos en un activo estrat√©gico que no solo gestione el presente, sino que tambi√©n permita a Ferrimac prepararse para el futuro, anticipando riesgos y capitalizando oportunidades de manera m√°s eficiente.