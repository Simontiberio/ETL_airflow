# :rocket: Proyecto ETL - Ferrimac :hammer: :wrench:

## :memo: Descripci√≥n del Proyecto

Este proyecto ETL (Extract, Transform, Load) est√° dise√±ado para Ferrimac, una ferreter√≠a que necesita optimizar la gesti√≥n de su stock. Utiliza Apache Airflow para automatizar el proceso de extracci√≥n, transformaci√≥n y carga de datos desde una API y archivos locales. El objetivo principal es calcular y visualizar el stock valorizado en pesos, tomando en cuenta los productos en inventario, su precio en d√≥lares y la cotizaci√≥n diaria del d√≥lar.

El pipeline incluye m√∫ltiples tareas (tasks) en Airflow, responsables de extraer la informaci√≥n de ventas y compras, actualizar el stock y cargar los datos transformados en una base de datos Amazon Redshift.


## Estructura del Proyecto 


```bash

ETL_AIRFLOW
    ‚îú‚îÄ‚îÄ /base_datos/                                   # Base de datos donde se almacenan archivos del negocio.
    ‚îú‚îÄ‚îÄ /config/                                       # Configuraci√≥n de Airflow.
    ‚îú‚îÄ‚îÄ /dags/
    ‚îÇ     ‚îú‚îÄ‚îÄ etl_update_stock_ferrimac.py             # Archivo principal del DAG de Airflow.
    ‚îú‚îÄ‚îÄ /logs/                                         # Carpeta donde persisten los logs de Airflow.
    ‚îú‚îÄ‚îÄ /functions_etl/
    ‚îÇ     ‚îú‚îÄ‚îÄ data_transform.py                        # Archivo que transforma los datos y valoriza el stock.
    ‚îÇ     ‚îú‚îÄ‚îÄ extract_file_purchases_to_stock.py       # Funci√≥n que extrae datos del sistema de compras.
    ‚îÇ     ‚îú‚îÄ‚îÄ extract_file_sells_to_stock.py           # Funci√≥n que extrae datos del sistema de ventas.
    ‚îÇ     ‚îú‚îÄ‚îÄ load_data.py                             # Funci√≥n que carga los datos en Redshift.
    ‚îÇ     ‚îú‚îÄ‚îÄ obtain_currency.py                       # Funci√≥n que obtiene la cotizaci√≥n del d√≥lar.
    ‚îÇ     ‚îú‚îÄ‚îÄ update_stock.py                          # Funci√≥n que actualiza el stock en unidades.
    ‚îú‚îÄ‚îÄ /plugins/                                      # Directorio de configuraci√≥n de Airflow.
    ‚îú‚îÄ‚îÄ /test/                                         # Directorio con pruebas unitarias.
    ‚îú‚îÄ‚îÄ /utils/
    ‚îÇ     ‚îú‚îÄ‚îÄ config.py                                # Archivo de configuraci√≥n de variables de entorno.
    ‚îú‚îÄ‚îÄ .env                                           # Archivo de configuraci√≥n y definici√≥n de variables de entorno.
    ‚îú‚îÄ‚îÄ .gitignore                                     # Archivo de configuraci√≥n de Git.
    ‚îú‚îÄ‚îÄ docker-compose.yml                             # Archivo de configuraci√≥n de Docker Compose.
    ‚îú‚îÄ‚îÄ README.md                                      # Documentaci√≥n del proyecto.
    ‚îî‚îÄ‚îÄ requirements.txt                               # Archivo con librer√≠as utilizadas en el proyecto.
```



## :desktop_computer: :gear: Pasos de Configuraci√≥n.

## :memo: Pre requisitos.

:white_check_mark: *Python 3.8 o superior* üêç


:white_check_mark: *Apache Airflow* üå¨Ô∏è


:white_check_mark: *Docker y Docker Compose* üê≥

## Pasos

1- Clonar el repositorio.

```bash
# Comando para clonar el repositorio
git clone git@github.com:Simontiberio/ETL_airflow.git

```

2- Crear un archivo .env con las configuraciones necesarias, incluyendo las credenciales de la API y las rutas de los archivos de ventas y compras.


```bash
# UID AIRFLOW.
AIRFLOW_UID=1000

# Credenciales Redshift.
REDSHIFT_USER= '2024_nombre_apellido'
REDSHIFT_PASSWORD='PASSWORD'
REDSHIFT_DB= 'NOMBRE_DB'
REDSHIFT_HOST='REDSHIFT_HOST'
REDSHIFT_PORT= PORT

# Variables de entorno.
REDSHIFT_SCHEMA = 'REDSHIFT_SCHEMA'
api_key = 'your_api_key'
compras_file = 'nombre_archivo.xlsx'
stock_file = 'nombre_archivo.xlsx'
ventas_file = 'nombre_archivo.xlsx'
monetized_stock_file = 'nombre_archivo.xlsx'
data_quotes = 'nombre_archivo.csv'
list_prices_file = 'nombre_archivo.xlsx'

```
3- Iniciar los servicios de Airflow utilizando Docker Compose:

```bash
docker-compose up -d
```

4- Acceder a la interfaz de Airflow en http://localhost:8080 y activar el DAG llamado etl_update_stock_ferrimac.


## Estructura del Pipeline 

El pipeline de Airflow se compone de tres fases principales:

:one: Extracci√≥n de Datos

En esta etapa, se recolectan los datos mediante tres tareas ejecutadas en paralelo:

``` def append_to_data_price():```: Obtiene diariamente la cotizaci√≥n del d√≥lar desde una API, actualizando un archivo hist√≥rico de cotizaciones.

``` def extract_file_sells_to_stock (): ``` Extrae las ventas diarias del sistema de ventas al cierre de cada jornada.

``` def extract_file_purchases_to_stock ():``` Recupera las compras de productos a distintos proveedores, actualizando el inventario con las compras recepcionadas.

:two: Transformaci√≥n de Datos

Aqu√≠ se transforman los datos para calcular el valor del stock.

``` def update_stock (): ```: Actualiza el stock diario considerando las ventas y compras registradas.

``` def monetize_stock ():``` Valoriza el stock actualizado utilizando la √∫ltima lista de precios en d√≥lares y la cotizaci√≥n del d√≠a.

:three: Carga de Datos

En esta √∫ltima etapa, los datos son almacenados y cargados en Redshift.

``` def load_data ():``` Convierte el stock valorizado a formato Parquet para un almacenamiento m√°s eficiente.

``` def load_data_to_Redshift ():``` Carga el archivo Parquet a la base de datos Redshift, ubicada en un cluster de AWS.


## Representacion grafica del DAG y sus dependencias entre tareas.


![Arquitectura y dependencias del Pipeline](./image.png)



## Diagrama Entidad Relacion de Ferrimac

A continuacion, se muestra como se modelo el subdomionio de mercaderias: 

![Diagrama Entidad Relacion ](./DER-Ferrimac.png)